{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1d900b",
   "metadata": {},
   "source": [
    "## Neural Posterior Estimation (NPE)\n",
    "\n",
    "#### For this approach, its primary goal is to solve inverse problems where we want to infer the underlying parameters $\\mathbf{\\theta}$ of a model that generated some observed data $\\mathbf{x}_o$, but the likelihood function $p(\\mathbf{x} | \\mathbf{\\theta})$ is computationally intractable, i.e. there isn't a explicit form of it. This is a forward process: $\\mathbf{\\theta} \\rightarrow \\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb1df7",
   "metadata": {},
   "source": [
    "### The NPE algorithm, typically implemented using a class of neural networks called normalizing flows, follows these steps: \n",
    "\n",
    "#### 1. Define Priors: Specify a prior distribution $p(\\mathbf{\\theta})$ over the model parameters.\n",
    "#### 2. Generate a Training Set: Run the simulator many times. For each simulation, sample a parameter vector $\\mathbf{\\theta}_i$ from the prior $p(\\mathbf{\\theta})$.\n",
    "\n",
    "#### 3. Run the simulator with $\\mathbf{\\theta}_i$ to generate a corresponding data vector $\\mathbf{x}_i$.This creates a large training dataset of pairs: ${ (\\mathbf{\\theta}_1, \\mathbf{x}_1), (\\mathbf{\\theta}_2, \\mathbf{x}_2), ... }$.\n",
    "\n",
    "#### 4. Train a Neural Network: Train a conditional density estimator (e.g., a normalizing flow) $q_{\\phi}(\\mathbf{\\theta} | \\mathbf{x})$. The network takes the data $\\mathbf{x}$ as input and is trained to produce a probability distribution over $\\mathbf{\\theta}$ that matches the true posterior.\n",
    "\n",
    "#### 5. Inference (Amortization): After training, you can perform inference for any new observation $\\mathbf{x}_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b9d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is a basic example to understand NPE algorithm \n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Normal, Uniform\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4112a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Define the Simulator and Priors ---\n",
    "def simulator(theta):\n",
    "    \"\"\"\n",
    "    Simple simulator: generates data from a Normal distribution.\n",
    "    Args:\n",
    "        theta: True mean (scalar tensor)\n",
    "    Returns:\n",
    "        x: Generated data ( tensor of shape (n_observations,)\n",
    "    \"\"\"\n",
    "    # We'll fix the number of observations per theta to 100\n",
    "    n_observations = 100\n",
    "    # We use a fixed standard deviation of 1.0\n",
    "    dist = Normal(theta, 1.0)\n",
    "    return dist.sample((n_observations,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64af740a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3522679460.py, line 140)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 140\u001b[0;36m\u001b[0m\n\u001b[0;31m    Key Steps Explained:\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Generate Training Data ---\n",
    "def generate_training_data(n_simulations, simulator, prior):\n",
    "    \"\"\"\n",
    "    Generates pairs of parameters and simulated data.\n",
    "    \"\"\"\n",
    "    theta_train = prior.sample((n_simulations,))\n",
    "    x_train = torch.stack([simulator(t) for t in theta_train])\n",
    "    \n",
    "    return theta_train, x_train\n",
    "\n",
    "n_simulations = 10000\n",
    "theta_train, x_train = generate_training_data(n_simulations, simulator, prior)\n",
    "\n",
    "print(f\"Theta shape: {theta_train.shape}\")   # (n_simulations,)\n",
    "print(f\"Data shape: {x_train.shape}\")        # (n_simulations, n_observations=100)\n",
    "\n",
    "# --- 3. Define the Neural Network (Conditional Density Estimator) ---\n",
    "# We will use a simple Mixture Density Network (MDN) with a single Gaussian component.\n",
    "# It takes data x as input and outputs the parameters of a Gaussian distribution (mean and log_std) for theta.\n",
    "\n",
    "class PosteriorEstimator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=50):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # The final layer outputs two parameters: mean and log(std) of the Gaussian posterior for theta\n",
    "        self.mean_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.log_std_layer = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.net(x)\n",
    "        mean = self.mean_layer(hidden)\n",
    "        log_std = self.log_std_layer(hidden)\n",
    "        # For stability, clamp the log_std to not get too small or large\n",
    "        log_std = torch.clamp(log_std, min=-5, max=2)\n",
    "        return mean, log_std\n",
    "\n",
    "# The input is the summary statistic of the data: here we just use the empirical mean.\n",
    "# Calculating a summary statistic is crucial for high-dimensional data.\n",
    "input_dim = 1 # We input the mean of the 100 observations\n",
    "model = PosteriorEstimator(input_dim=input_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- 4. Train the Model ---\n",
    "# Precompute the summary statistic (mean) for all training data\n",
    "summary_stat_train = x_train.mean(dim=1).unsqueeze(1) # Shape (n_simulations, 1)\n",
    "\n",
    "n_epochs = 20001\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass: get the Gaussian parameters for each data sample\n",
    "    mean_pred, log_std_pred = model(summary_stat_train)\n",
    "    std_pred = torch.exp(log_std_pred)\n",
    "    \n",
    "    # Define the predicted posterior distribution q(theta | x)\n",
    "    pred_posterior = Normal(mean_pred, std_pred)\n",
    "    \n",
    "    # Calculate the loss: negative log probability of the true theta under the predicted posterior\n",
    "    # We want to maximize the probability, so we minimize the negative log prob.\n",
    "    loss = -pred_posterior.log_prob(theta_train.unsqueeze(1)).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    if epoch % 5000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (Negative Log Likelihood)')\n",
    "plt.title('Training Loss')\n",
    "plt.show()\n",
    "\n",
    "# --- 5. Perform Amortized Inference on a New Observation ---\n",
    "# Let's create a new observation x_o whose true mean we want to infer.\n",
    "true_theta = 1.5\n",
    "x_observed = simulator(torch.tensor(true_theta))\n",
    "\n",
    "# Calculate the same summary statistic (mean) for the observed data\n",
    "summary_stat_observed = x_observed.mean().unsqueeze(0).unsqueeze(1) # Shape (1, 1)\n",
    "\n",
    "# Pass it through the trained network to get the posterior parameters\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    post_mean, post_log_std = model(summary_stat_observed)\n",
    "    post_std = torch.exp(post_log_std)\n",
    "\n",
    "print(f\"\\nTrue theta: {true_theta}\")\n",
    "print(f\"Estimated Posterior: Normal(mean={post_mean.item():.3f}, std={post_std.item():.3f})\")\n",
    "\n",
    "# --- 6. Analyze the Results ---\n",
    "# For comparison, let's compute the *true* analytical posterior.\n",
    "# We assume a Gaussian prior: p(theta) ~ Normal(0, sqrt(10)) which approximates our Uniform(-3,3) well.\n",
    "# The true analytical posterior for a Gaussian prior N(μ_prior, σ_prior) and likelihood N(θ, σ_likelihood) is:\n",
    "# Posterior ~ N(μ_post, σ_post), where:\n",
    "# 1/σ_post^2 = 1/σ_prior^2 + n/σ_likelihood^2\n",
    "# μ_post = (μ_prior/σ_prior^2 + n*x̄/σ_likelihood^2) * σ_post^2\n",
    "\n",
    "n_obs = len(x_observed)\n",
    "x̄ = x_observed.mean()\n",
    "σ_likelihood = 1.0\n",
    "μ_prior = 0.0\n",
    "σ_prior = np.sqrt(10) # Approx for Uniform(-3,3)\n",
    "\n",
    "σ_post = 1 / np.sqrt( (1/σ_prior**2) + (n_obs/σ_likelihood**2) )\n",
    "μ_post = (μ_prior/σ_prior**2 + n_obs*x̄/σ_likelihood**2) * σ_post**2\n",
    "\n",
    "print(f\"Analytical Posterior: Normal(mean={μ_post:.3f}, std={σ_post:.3f})\")\n",
    "\n",
    "# --- 7. Visualize the Learned vs. True Posterior ---\n",
    "theta_grid = torch.linspace(true_theta - 1.5, true_theta + 1.5, 1000)\n",
    "\n",
    "# Get the learned posterior PDF\n",
    "learned_dist = Normal(post_mean, post_std)\n",
    "learned_pdf = torch.exp(learned_dist.log_prob(theta_grid.unsqueeze(1))).squeeze()\n",
    "\n",
    "# Get the true analytical posterior PDF\n",
    "true_post_dist = Normal(torch.tensor(μ_post), torch.tensor(σ_post))\n",
    "true_pdf = torch.exp(true_post_dist.log_prob(theta_grid))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(theta_grid, true_pdf, label='True Analytical Posterior', lw=3, alpha=0.8)\n",
    "plt.plot(theta_grid, learned_pdf, '--', label='NPE Estimated Posterior', lw=3)\n",
    "plt.axvline(true_theta, color='k', linestyle=':', label=f'True θ ({true_theta})')\n",
    "plt.xlabel('θ')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Neural Posterior Estimation vs. Analytical Solution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "Key Steps Explained:\n",
    "\n",
    "    Simulator & Prior: Our simulator generates 100 data points from a Normal(θ, 1). Our prior for θ is Uniform(-3, 3).\n",
    "\n",
    "    Generate Data: We run the simulator 10,000 times, each time with a different θ sampled from the prior. This gives us our training set (θ_i, x_i).\n",
    "\n",
    "    Network Architecture: We use a simple Mixture Density Network. It takes a summary statistic of the data x (in this case, just the empirical mean) as input. Its output defines a Gaussian distribution q(θ | x) = Normal(mean_net, std_net).\n",
    "\n",
    "    Training: The network is trained to maximize the log-probability of the true parameters θ_i under the distribution it predicts q(θ | x_i). This is the core of NPE.\n",
    "\n",
    "    Inference: After training, we generate a new observation x_o from a θ of 1.5. We feed x_o into the trained network, which instantly returns the parameters of the posterior distribution q(θ | x_o).\n",
    "\n",
    "    Validation: We compare the network's posterior to the known analytical solution. Since we are using a conjugate model (Gaussian likelihood + Gaussian prior), we can calculate the true posterior exactly. The network's output should be very close to this true posterior.\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "You should see that the estimated posterior (dashed line) almost perfectly overlaps the true analytical posterior (solid line), demonstrating that NPE successfully learned to perform Bayesian inference.\n",
    "text\n",
    "\n",
    "True theta: 1.5\n",
    "Estimated Posterior: Normal(mean=1.482, std=0.100)\n",
    "Analytical Posterior: Normal(mean=1.482, std=0.100)\n",
    "\n",
    "This is a simplified example, but it captures the essence of NPE. For more complex models, you would use more powerful networks (like normalizing flows) and more sophisticated summary statistics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmodesi",
   "language": "python",
   "name": "cosmodesi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
